{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blood Cells Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "import tensorflow as tf \n",
    "from keras import backend as k\n",
    "from keras.models import load_model\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "import scipy.io as sio\n",
    "from skimage import io\n",
    "from random import shuffle\n",
    "from scipy import misc\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_data_2d():\n",
    "    \n",
    "    EOSINOPHIL = os.listdir(\"TEST/EOSINOPHIL/\")\n",
    "    LYMPHOCYTE = os.listdir(\"TEST/LYMPHOCYTE/\")\n",
    "    MONOCYTE   = os.listdir(\"TEST/MONOCYTE/\")\n",
    "    NEUTROPHIL = os.listdir(\"TEST/NEUTROPHIL/\")\n",
    "    \n",
    "\n",
    "    testing_data = []\n",
    "    \n",
    "\n",
    "    for (imgE, imgL, imgM, imgN) in tqdm(zip(EOSINOPHIL, LYMPHOCYTE, MONOCYTE, NEUTROPHIL)):\n",
    "\n",
    "        img_e = io.imread('TEST/EOSINOPHIL/' + imgE , as_grey=True)\n",
    "        img_l = io.imread('TEST/LYMPHOCYTE/' + imgL , as_grey=True)\n",
    "        img_m = io.imread('TEST/MONOCYTE/'   + imgM , as_grey=True)\n",
    "        img_n = io.imread('TEST/NEUTROPHIL/' + imgN , as_grey=True)\n",
    "        \n",
    "        img_e = misc.imresize(img_e , (240, 240))\n",
    "        img_l = misc.imresize(img_l , (240, 240))\n",
    "        img_m = misc.imresize(img_m , (240, 240))\n",
    "        img_n = misc.imresize(img_n , (240, 240))\n",
    "        \n",
    "        testing_data.append([np.array(img_e), 1])\n",
    "        testing_data.append([np.array(img_l), 2])\n",
    "        testing_data.append([np.array(img_m), 3])\n",
    "        testing_data.append([np.array(img_n), 4])\n",
    "        \n",
    "    #imgs = np.array(imgs)\n",
    "\n",
    "    #images = imgs[:,0]\n",
    "    #labels = imgs[:,1]\n",
    "            \n",
    "    #return images,labels\n",
    "    \n",
    "    shuffle(testing_data)\n",
    "    \n",
    "    return testing_data\n",
    "\n",
    "def get_training_data_2d():\n",
    "    \n",
    "    EOSINOPHIL = os.listdir(\"TRAIN/EOSINOPHIL/\")\n",
    "    LYMPHOCYTE = os.listdir(\"TRAIN/LYMPHOCYTE/\")\n",
    "    MONOCYTE   = os.listdir(\"TRAIN/MONOCYTE/\")\n",
    "    NEUTROPHIL = os.listdir(\"TRAIN/NEUTROPHIL/\")\n",
    "    \n",
    "\n",
    "    training_data = []\n",
    "    \n",
    "\n",
    "    for (imgE, imgL, imgM, imgN) in tqdm(zip(EOSINOPHIL, LYMPHOCYTE, MONOCYTE, NEUTROPHIL)):\n",
    "\n",
    "        img_e = io.imread('TRAIN/EOSINOPHIL/' + imgE , as_grey=True)\n",
    "        img_l = io.imread('TRAIN/LYMPHOCYTE/' + imgL , as_grey=True)\n",
    "        img_m = io.imread('TRAIN/MONOCYTE/'   + imgM , as_grey=True)\n",
    "        img_n = io.imread('TRAIN/NEUTROPHIL/' + imgN , as_grey=True)\n",
    "        \n",
    "        img_e = misc.imresize(img_e , (240, 240))\n",
    "        img_l = misc.imresize(img_l , (240, 240))\n",
    "        img_m = misc.imresize(img_m , (240, 240))\n",
    "        img_n = misc.imresize(img_n , (240, 240))\n",
    "        \n",
    "        training_data.append([np.array(img_e), 1])\n",
    "        training_data.append([np.array(img_l), 2])\n",
    "        training_data.append([np.array(img_m), 3])\n",
    "        training_data.append([np.array(img_n), 4])\n",
    "        \n",
    "    #imgs = np.array(imgs)\n",
    "\n",
    "    #images = imgs[:,0]\n",
    "    #labels = imgs[:,1]\n",
    "            \n",
    "    #return images,labels\n",
    "    \n",
    "    shuffle(training_data)\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:58: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:59: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:60: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "2478it [04:48,  8.59it/s]\n",
      "0it [00:00, ?it/s]C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "620it [01:03,  9.73it/s]\n"
     ]
    }
   ],
   "source": [
    "training_data = get_training_data_2d()\n",
    "testing_data  = get_testing_data_2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([i[0] for i in training_data]).reshape(-1,240,240,1)\n",
    "y_train = [i[1] for i in training_data]\n",
    "\n",
    "x_test = np.array([i[0] for i in testing_data]).reshape(-1,240,240,1)\n",
    "y_test = [i[1] for i in testing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "nr_classes = 5\n",
    "nr_iterations = 5\n",
    "saved_weights_name='SVMWeights.h5'\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "\n",
    "x_test /= 255\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, nr_classes)\n",
    "\n",
    "y_test = np_utils.to_categorical(y_test, nr_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train[:,1:5]\n",
    "y_test = y_test[:,1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating model using Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 236, 236, 32)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 47, 47, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 43, 43, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 577,028\n",
      "Trainable params: 577,028\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape=(240,240,1)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(5, 5)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(5, 5)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "saved_weights_name='CNNWeights.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(saved_weights_name, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "csv_logger = CSVLogger('v.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9912 samples, validate on 2480 samples\n",
      "Epoch 1/5\n",
      "9912/9912 [==============================] - ETA: 13:35 - loss: 5.8922 - acc: 0.25 - ETA: 12:14 - loss: 3.8559 - acc: 0.27 - ETA: 11:46 - loss: 3.0573 - acc: 0.27 - ETA: 11:21 - loss: 2.6673 - acc: 0.26 - ETA: 11:19 - loss: 2.4134 - acc: 0.26 - ETA: 11:15 - loss: 2.2443 - acc: 0.26 - ETA: 11:12 - loss: 2.1230 - acc: 0.25 - ETA: 11:12 - loss: 2.0306 - acc: 0.25 - ETA: 11:08 - loss: 1.9596 - acc: 0.25 - ETA: 11:05 - loss: 1.9014 - acc: 0.25 - ETA: 11:00 - loss: 1.8536 - acc: 0.25 - ETA: 10:55 - loss: 1.8134 - acc: 0.26 - ETA: 10:52 - loss: 1.7850 - acc: 0.26 - ETA: 10:48 - loss: 1.7571 - acc: 0.26 - ETA: 10:46 - loss: 1.7322 - acc: 0.26 - ETA: 10:44 - loss: 1.7101 - acc: 0.26 - ETA: 10:37 - loss: 1.6907 - acc: 0.26 - ETA: 10:29 - loss: 1.6738 - acc: 0.27 - ETA: 10:20 - loss: 1.6584 - acc: 0.27 - ETA: 10:10 - loss: 1.6444 - acc: 0.27 - ETA: 10:02 - loss: 1.6322 - acc: 0.27 - ETA: 9:55 - loss: 1.6213 - acc: 0.2709 - ETA: 9:47 - loss: 1.6112 - acc: 0.268 - ETA: 9:38 - loss: 1.6018 - acc: 0.270 - ETA: 9:31 - loss: 1.5930 - acc: 0.271 - ETA: 9:24 - loss: 1.5852 - acc: 0.270 - ETA: 9:15 - loss: 1.5782 - acc: 0.268 - ETA: 9:07 - loss: 1.5714 - acc: 0.268 - ETA: 9:00 - loss: 1.5650 - acc: 0.268 - ETA: 8:53 - loss: 1.5596 - acc: 0.265 - ETA: 8:45 - loss: 1.5540 - acc: 0.264 - ETA: 8:37 - loss: 1.5488 - acc: 0.263 - ETA: 8:30 - loss: 1.5440 - acc: 0.259 - ETA: 8:22 - loss: 1.5395 - acc: 0.257 - ETA: 8:14 - loss: 1.5352 - acc: 0.255 - ETA: 8:07 - loss: 1.5312 - acc: 0.254 - ETA: 7:59 - loss: 1.5273 - acc: 0.255 - ETA: 7:51 - loss: 1.5238 - acc: 0.254 - ETA: 7:43 - loss: 1.5203 - acc: 0.254 - ETA: 7:36 - loss: 1.5167 - acc: 0.255 - ETA: 7:28 - loss: 1.5140 - acc: 0.254 - ETA: 7:20 - loss: 1.5110 - acc: 0.254 - ETA: 7:12 - loss: 1.5080 - acc: 0.256 - ETA: 7:04 - loss: 1.5055 - acc: 0.255 - ETA: 6:57 - loss: 1.5028 - acc: 0.257 - ETA: 6:48 - loss: 1.5003 - acc: 0.256 - ETA: 6:41 - loss: 1.4979 - acc: 0.256 - ETA: 6:33 - loss: 1.4957 - acc: 0.256 - ETA: 6:26 - loss: 1.4935 - acc: 0.255 - ETA: 6:19 - loss: 1.4913 - acc: 0.257 - ETA: 6:11 - loss: 1.4890 - acc: 0.259 - ETA: 6:04 - loss: 1.4874 - acc: 0.258 - ETA: 5:57 - loss: 1.4857 - acc: 0.258 - ETA: 5:49 - loss: 1.4838 - acc: 0.258 - ETA: 5:41 - loss: 1.4821 - acc: 0.258 - ETA: 5:33 - loss: 1.4804 - acc: 0.258 - ETA: 5:25 - loss: 1.4789 - acc: 0.257 - ETA: 5:18 - loss: 1.4774 - acc: 0.257 - ETA: 5:10 - loss: 1.4760 - acc: 0.257 - ETA: 5:02 - loss: 1.4744 - acc: 0.258 - ETA: 4:54 - loss: 1.4731 - acc: 0.258 - ETA: 4:47 - loss: 1.4716 - acc: 0.258 - ETA: 4:39 - loss: 1.4704 - acc: 0.257 - ETA: 4:31 - loss: 1.4690 - acc: 0.258 - ETA: 4:24 - loss: 1.4679 - acc: 0.258 - ETA: 4:16 - loss: 1.4668 - acc: 0.257 - ETA: 4:08 - loss: 1.4655 - acc: 0.258 - ETA: 4:00 - loss: 1.4644 - acc: 0.259 - ETA: 3:53 - loss: 1.4633 - acc: 0.259 - ETA: 3:45 - loss: 1.4621 - acc: 0.259 - ETA: 3:37 - loss: 1.4611 - acc: 0.258 - ETA: 3:30 - loss: 1.4602 - acc: 0.257 - ETA: 3:22 - loss: 1.4592 - acc: 0.257 - ETA: 3:15 - loss: 1.4582 - acc: 0.257 - ETA: 3:08 - loss: 1.4572 - acc: 0.257 - ETA: 3:00 - loss: 1.4564 - acc: 0.257 - ETA: 2:52 - loss: 1.4556 - acc: 0.256 - ETA: 2:44 - loss: 1.4550 - acc: 0.255 - ETA: 2:36 - loss: 1.4541 - acc: 0.255 - ETA: 2:28 - loss: 1.4534 - acc: 0.255 - ETA: 2:21 - loss: 1.4526 - acc: 0.254 - ETA: 2:13 - loss: 1.4517 - acc: 0.253 - ETA: 2:06 - loss: 1.4511 - acc: 0.253 - ETA: 1:58 - loss: 1.4503 - acc: 0.253 - ETA: 1:50 - loss: 1.4496 - acc: 0.253 - ETA: 1:42 - loss: 1.4488 - acc: 0.254 - ETA: 1:35 - loss: 1.4480 - acc: 0.254 - ETA: 1:27 - loss: 1.4478 - acc: 0.252 - ETA: 1:19 - loss: 1.4472 - acc: 0.252 - ETA: 1:11 - loss: 1.4465 - acc: 0.252 - ETA: 1:03 - loss: 1.4459 - acc: 0.251 - ETA: 55s - loss: 1.4453 - acc: 0.251 - ETA: 47s - loss: 1.4448 - acc: 0.25 - ETA: 40s - loss: 1.4442 - acc: 0.25 - ETA: 32s - loss: 1.4435 - acc: 0.25 - ETA: 24s - loss: 1.4429 - acc: 0.25 - ETA: 16s - loss: 1.4424 - acc: 0.25 - ETA: 8s - loss: 1.4418 - acc: 0.2510 - ETA: 0s - loss: 1.4413 - acc: 0.250 - 853s 86ms/step - loss: 1.4413 - acc: 0.2508 - val_loss: 1.3890 - val_acc: 0.2500\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.25000, saving model to CNNWeights.h5\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9912/9912 [==============================] - ETA: 12:37 - loss: 1.3835 - acc: 0.26 - ETA: 12:23 - loss: 1.3976 - acc: 0.24 - ETA: 12:28 - loss: 1.3945 - acc: 0.24 - ETA: 12:16 - loss: 1.3941 - acc: 0.23 - ETA: 12:07 - loss: 1.3933 - acc: 0.24 - ETA: 12:07 - loss: 1.3899 - acc: 0.26 - ETA: 11:58 - loss: 1.3921 - acc: 0.25 - ETA: 11:48 - loss: 1.3939 - acc: 0.24 - ETA: 11:38 - loss: 1.3927 - acc: 0.24 - ETA: 11:28 - loss: 1.3931 - acc: 0.24 - ETA: 11:20 - loss: 1.3927 - acc: 0.24 - ETA: 11:11 - loss: 1.3923 - acc: 0.24 - ETA: 11:03 - loss: 1.3905 - acc: 0.25 - ETA: 10:55 - loss: 1.3904 - acc: 0.25 - ETA: 10:48 - loss: 1.3915 - acc: 0.25 - ETA: 10:41 - loss: 1.3910 - acc: 0.25 - ETA: 10:34 - loss: 1.3907 - acc: 0.25 - ETA: 10:30 - loss: 1.3902 - acc: 0.25 - ETA: 10:23 - loss: 1.3906 - acc: 0.25 - ETA: 10:15 - loss: 1.3907 - acc: 0.25 - ETA: 10:08 - loss: 1.3901 - acc: 0.25 - ETA: 10:01 - loss: 1.3900 - acc: 0.25 - ETA: 9:53 - loss: 1.3901 - acc: 0.2552 - ETA: 9:45 - loss: 1.3901 - acc: 0.252 - ETA: 9:37 - loss: 1.3905 - acc: 0.250 - ETA: 9:29 - loss: 1.3906 - acc: 0.250 - ETA: 9:21 - loss: 1.3904 - acc: 0.251 - ETA: 9:13 - loss: 1.3902 - acc: 0.252 - ETA: 9:05 - loss: 1.3905 - acc: 0.251 - ETA: 8:57 - loss: 1.3903 - acc: 0.251 - ETA: 8:48 - loss: 1.3900 - acc: 0.251 - ETA: 8:40 - loss: 1.3899 - acc: 0.249 - ETA: 8:32 - loss: 1.3898 - acc: 0.250 - ETA: 8:24 - loss: 1.3897 - acc: 0.251 - ETA: 8:16 - loss: 1.3898 - acc: 0.250 - ETA: 8:08 - loss: 1.3896 - acc: 0.252 - ETA: 8:00 - loss: 1.3897 - acc: 0.251 - ETA: 7:52 - loss: 1.3897 - acc: 0.252 - ETA: 7:45 - loss: 1.3899 - acc: 0.251 - ETA: 7:37 - loss: 1.3899 - acc: 0.251 - ETA: 7:29 - loss: 1.3899 - acc: 0.249 - ETA: 7:22 - loss: 1.3900 - acc: 0.249 - ETA: 7:14 - loss: 1.3899 - acc: 0.248 - ETA: 7:06 - loss: 1.3897 - acc: 0.250 - ETA: 6:58 - loss: 1.3899 - acc: 0.249 - ETA: 6:51 - loss: 1.3898 - acc: 0.251 - ETA: 6:43 - loss: 1.3899 - acc: 0.250 - ETA: 6:35 - loss: 1.3902 - acc: 0.249 - ETA: 6:28 - loss: 1.3902 - acc: 0.249 - ETA: 6:21 - loss: 1.3901 - acc: 0.249 - ETA: 6:14 - loss: 1.3900 - acc: 0.250 - ETA: 6:06 - loss: 1.3899 - acc: 0.251 - ETA: 5:58 - loss: 1.3901 - acc: 0.250 - ETA: 5:50 - loss: 1.3902 - acc: 0.249 - ETA: 5:42 - loss: 1.3903 - acc: 0.248 - ETA: 5:34 - loss: 1.3903 - acc: 0.247 - ETA: 5:27 - loss: 1.3902 - acc: 0.249 - ETA: 5:19 - loss: 1.3901 - acc: 0.249 - ETA: 5:12 - loss: 1.3900 - acc: 0.249 - ETA: 5:04 - loss: 1.3899 - acc: 0.248 - ETA: 4:56 - loss: 1.3901 - acc: 0.248 - ETA: 4:48 - loss: 1.3899 - acc: 0.249 - ETA: 4:40 - loss: 1.3903 - acc: 0.249 - ETA: 4:32 - loss: 1.3905 - acc: 0.248 - ETA: 4:25 - loss: 1.3904 - acc: 0.249 - ETA: 4:17 - loss: 1.3904 - acc: 0.248 - ETA: 4:09 - loss: 1.3905 - acc: 0.248 - ETA: 4:01 - loss: 1.3904 - acc: 0.249 - ETA: 3:53 - loss: 1.3903 - acc: 0.249 - ETA: 3:46 - loss: 1.3903 - acc: 0.249 - ETA: 3:38 - loss: 1.3904 - acc: 0.249 - ETA: 3:30 - loss: 1.3903 - acc: 0.249 - ETA: 3:22 - loss: 1.3902 - acc: 0.250 - ETA: 3:14 - loss: 1.3903 - acc: 0.249 - ETA: 3:07 - loss: 1.3901 - acc: 0.249 - ETA: 2:59 - loss: 1.3903 - acc: 0.250 - ETA: 2:51 - loss: 1.3903 - acc: 0.250 - ETA: 2:43 - loss: 1.3903 - acc: 0.250 - ETA: 2:36 - loss: 1.3904 - acc: 0.249 - ETA: 2:28 - loss: 1.3904 - acc: 0.248 - ETA: 2:21 - loss: 1.3903 - acc: 0.249 - ETA: 2:13 - loss: 1.3903 - acc: 0.249 - ETA: 2:05 - loss: 1.3904 - acc: 0.249 - ETA: 1:57 - loss: 1.3903 - acc: 0.250 - ETA: 1:49 - loss: 1.3904 - acc: 0.250 - ETA: 1:42 - loss: 1.3904 - acc: 0.250 - ETA: 1:34 - loss: 1.3904 - acc: 0.249 - ETA: 1:26 - loss: 1.3903 - acc: 0.250 - ETA: 1:18 - loss: 1.3905 - acc: 0.249 - ETA: 1:10 - loss: 1.3904 - acc: 0.249 - ETA: 1:03 - loss: 1.3904 - acc: 0.250 - ETA: 55s - loss: 1.3903 - acc: 0.249 - ETA: 47s - loss: 1.3902 - acc: 0.25 - ETA: 39s - loss: 1.3901 - acc: 0.25 - ETA: 32s - loss: 1.3901 - acc: 0.25 - ETA: 24s - loss: 1.3901 - acc: 0.25 - ETA: 16s - loss: 1.3902 - acc: 0.24 - ETA: 8s - loss: 1.3904 - acc: 0.2495 - ETA: 0s - loss: 1.3904 - acc: 0.249 - 847s 85ms/step - loss: 1.3904 - acc: 0.2488 - val_loss: 1.3932 - val_acc: 0.2500\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.25000 to 0.25000, saving model to CNNWeights.h5\n",
      "Epoch 3/5\n",
      "9912/9912 [==============================] - ETA: 12:46 - loss: 1.3792 - acc: 0.28 - ETA: 12:41 - loss: 1.3841 - acc: 0.28 - ETA: 12:31 - loss: 1.3852 - acc: 0.27 - ETA: 12:20 - loss: 1.3889 - acc: 0.26 - ETA: 12:10 - loss: 1.3876 - acc: 0.25 - ETA: 12:01 - loss: 1.3886 - acc: 0.25 - ETA: 11:51 - loss: 1.3874 - acc: 0.25 - ETA: 11:42 - loss: 1.3873 - acc: 0.25 - ETA: 11:33 - loss: 1.3884 - acc: 0.25 - ETA: 11:29 - loss: 1.3884 - acc: 0.25 - ETA: 11:20 - loss: 1.3883 - acc: 0.25 - ETA: 11:11 - loss: 1.3879 - acc: 0.26 - ETA: 11:03 - loss: 1.3875 - acc: 0.26 - ETA: 10:55 - loss: 1.3866 - acc: 0.26 - ETA: 10:46 - loss: 1.3881 - acc: 0.25 - ETA: 10:38 - loss: 1.3884 - acc: 0.25 - ETA: 10:30 - loss: 1.3882 - acc: 0.25 - ETA: 10:22 - loss: 1.3881 - acc: 0.25 - ETA: 10:14 - loss: 1.3881 - acc: 0.25 - ETA: 10:06 - loss: 1.3878 - acc: 0.25 - ETA: 9:58 - loss: 1.3881 - acc: 0.2533 - ETA: 9:50 - loss: 1.3881 - acc: 0.252 - ETA: 9:42 - loss: 1.3883 - acc: 0.253 - ETA: 9:35 - loss: 1.3882 - acc: 0.257 - ETA: 9:28 - loss: 1.3882 - acc: 0.257 - ETA: 9:21 - loss: 1.3882 - acc: 0.257 - ETA: 9:14 - loss: 1.3883 - acc: 0.254 - ETA: 9:07 - loss: 1.3882 - acc: 0.254 - ETA: 9:00 - loss: 1.3882 - acc: 0.255 - ETA: 8:53 - loss: 1.3885 - acc: 0.254 - ETA: 8:46 - loss: 1.3886 - acc: 0.252 - ETA: 8:38 - loss: 1.3886 - acc: 0.253 - ETA: 8:31 - loss: 1.3884 - acc: 0.256 - ETA: 8:23 - loss: 1.3884 - acc: 0.256 - ETA: 8:15 - loss: 1.3883 - acc: 0.258 - ETA: 8:07 - loss: 1.3884 - acc: 0.257 - ETA: 8:00 - loss: 1.3882 - acc: 0.258 - ETA: 7:52 - loss: 1.3878 - acc: 0.259 - ETA: 7:45 - loss: 1.3883 - acc: 0.258 - ETA: 7:37 - loss: 1.3884 - acc: 0.258 - ETA: 7:29 - loss: 1.3882 - acc: 0.259 - ETA: 7:21 - loss: 1.3882 - acc: 0.259 - ETA: 7:13 - loss: 1.3882 - acc: 0.259 - ETA: 7:06 - loss: 1.3883 - acc: 0.258 - ETA: 6:58 - loss: 1.3884 - acc: 0.256 - ETA: 6:50 - loss: 1.3883 - acc: 0.256 - ETA: 6:42 - loss: 1.3884 - acc: 0.254 - ETA: 6:34 - loss: 1.3884 - acc: 0.254 - ETA: 6:28 - loss: 1.3884 - acc: 0.254 - ETA: 6:20 - loss: 1.3884 - acc: 0.255 - ETA: 6:12 - loss: 1.3883 - acc: 0.255 - ETA: 6:04 - loss: 1.3884 - acc: 0.254 - ETA: 5:57 - loss: 1.3883 - acc: 0.255 - ETA: 5:49 - loss: 1.3882 - acc: 0.255 - ETA: 5:41 - loss: 1.3881 - acc: 0.255 - ETA: 5:33 - loss: 1.3881 - acc: 0.256 - ETA: 5:25 - loss: 1.3881 - acc: 0.257 - ETA: 5:18 - loss: 1.3880 - acc: 0.257 - ETA: 5:10 - loss: 1.3881 - acc: 0.256 - ETA: 5:02 - loss: 1.3883 - acc: 0.255 - ETA: 4:54 - loss: 1.3884 - acc: 0.254 - ETA: 4:46 - loss: 1.3886 - acc: 0.254 - ETA: 4:39 - loss: 1.3886 - acc: 0.253 - ETA: 4:31 - loss: 1.3885 - acc: 0.255 - ETA: 4:23 - loss: 1.3886 - acc: 0.254 - ETA: 4:16 - loss: 1.3884 - acc: 0.255 - ETA: 4:08 - loss: 1.3885 - acc: 0.254 - ETA: 4:00 - loss: 1.3887 - acc: 0.254 - ETA: 3:53 - loss: 1.3887 - acc: 0.254 - ETA: 3:45 - loss: 1.3886 - acc: 0.253 - ETA: 3:38 - loss: 1.3886 - acc: 0.253 - ETA: 3:30 - loss: 1.3886 - acc: 0.253 - ETA: 3:22 - loss: 1.3884 - acc: 0.255 - ETA: 3:14 - loss: 1.3882 - acc: 0.255 - ETA: 3:07 - loss: 1.3883 - acc: 0.255 - ETA: 2:59 - loss: 1.3883 - acc: 0.255 - ETA: 2:51 - loss: 1.3881 - acc: 0.256 - ETA: 2:43 - loss: 1.3882 - acc: 0.255 - ETA: 2:36 - loss: 1.3882 - acc: 0.255 - ETA: 2:28 - loss: 1.3883 - acc: 0.255 - ETA: 2:20 - loss: 1.3882 - acc: 0.254 - ETA: 2:12 - loss: 1.3882 - acc: 0.254 - ETA: 2:04 - loss: 1.3883 - acc: 0.253 - ETA: 1:57 - loss: 1.3884 - acc: 0.253 - ETA: 1:49 - loss: 1.3883 - acc: 0.254 - ETA: 1:41 - loss: 1.3885 - acc: 0.253 - ETA: 1:34 - loss: 1.3885 - acc: 0.253 - ETA: 1:26 - loss: 1.3885 - acc: 0.252 - ETA: 1:18 - loss: 1.3884 - acc: 0.252 - ETA: 1:11 - loss: 1.3884 - acc: 0.253 - ETA: 1:03 - loss: 1.3884 - acc: 0.252 - ETA: 55s - loss: 1.3885 - acc: 0.252 - ETA: 47s - loss: 1.3885 - acc: 0.25 - ETA: 39s - loss: 1.3884 - acc: 0.25 - ETA: 32s - loss: 1.3883 - acc: 0.25 - ETA: 24s - loss: 1.3881 - acc: 0.25 - ETA: 16s - loss: 1.3884 - acc: 0.25 - ETA: 8s - loss: 1.3884 - acc: 0.2528 - ETA: 0s - loss: 1.3884 - acc: 0.253 - 854s 86ms/step - loss: 1.3884 - acc: 0.2533 - val_loss: 1.3967 - val_acc: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_acc improved from 0.25000 to 0.25000, saving model to CNNWeights.h5\n",
      "Epoch 4/5\n",
      "9912/9912 [==============================] - ETA: 14:09 - loss: 1.4156 - acc: 0.20 - ETA: 13:46 - loss: 1.4060 - acc: 0.20 - ETA: 13:41 - loss: 1.3976 - acc: 0.25 - ETA: 13:39 - loss: 1.3970 - acc: 0.25 - ETA: 13:29 - loss: 1.3944 - acc: 0.25 - ETA: 13:11 - loss: 1.3917 - acc: 0.26 - ETA: 12:56 - loss: 1.3921 - acc: 0.26 - ETA: 12:42 - loss: 1.3926 - acc: 0.25 - ETA: 12:31 - loss: 1.3914 - acc: 0.26 - ETA: 12:20 - loss: 1.3905 - acc: 0.26 - ETA: 12:10 - loss: 1.3900 - acc: 0.26 - ETA: 12:01 - loss: 1.3888 - acc: 0.26 - ETA: 11:51 - loss: 1.3891 - acc: 0.26 - ETA: 11:40 - loss: 1.3887 - acc: 0.26 - ETA: 11:31 - loss: 1.3884 - acc: 0.26 - ETA: 11:21 - loss: 1.3885 - acc: 0.26 - ETA: 11:11 - loss: 1.3887 - acc: 0.26 - ETA: 11:01 - loss: 1.3885 - acc: 0.26 - ETA: 10:51 - loss: 1.3885 - acc: 0.26 - ETA: 10:41 - loss: 1.3887 - acc: 0.25 - ETA: 10:31 - loss: 1.3885 - acc: 0.25 - ETA: 10:22 - loss: 1.3882 - acc: 0.25 - ETA: 10:12 - loss: 1.3883 - acc: 0.25 - ETA: 10:03 - loss: 1.3883 - acc: 0.25 - ETA: 9:54 - loss: 1.3883 - acc: 0.2580 - ETA: 9:46 - loss: 1.3882 - acc: 0.257 - ETA: 9:37 - loss: 1.3880 - acc: 0.258 - ETA: 9:28 - loss: 1.3880 - acc: 0.258 - ETA: 9:20 - loss: 1.3877 - acc: 0.259 - ETA: 9:11 - loss: 1.3876 - acc: 0.259 - ETA: 9:02 - loss: 1.3879 - acc: 0.257 - ETA: 8:54 - loss: 1.3879 - acc: 0.256 - ETA: 8:46 - loss: 1.3879 - acc: 0.257 - ETA: 8:39 - loss: 1.3878 - acc: 0.256 - ETA: 8:31 - loss: 1.3877 - acc: 0.255 - ETA: 8:23 - loss: 1.3876 - acc: 0.257 - ETA: 8:16 - loss: 1.3876 - acc: 0.256 - ETA: 8:08 - loss: 1.3878 - acc: 0.256 - ETA: 8:00 - loss: 1.3877 - acc: 0.257 - ETA: 7:52 - loss: 1.3876 - acc: 0.258 - ETA: 7:44 - loss: 1.3877 - acc: 0.257 - ETA: 7:37 - loss: 1.3877 - acc: 0.256 - ETA: 7:28 - loss: 1.3877 - acc: 0.255 - ETA: 7:20 - loss: 1.3876 - acc: 0.255 - ETA: 7:12 - loss: 1.3873 - acc: 0.255 - ETA: 7:04 - loss: 1.3878 - acc: 0.253 - ETA: 6:56 - loss: 1.3878 - acc: 0.253 - ETA: 6:48 - loss: 1.3876 - acc: 0.254 - ETA: 6:40 - loss: 1.3876 - acc: 0.253 - ETA: 6:32 - loss: 1.3878 - acc: 0.252 - ETA: 6:24 - loss: 1.3880 - acc: 0.250 - ETA: 6:16 - loss: 1.3879 - acc: 0.250 - ETA: 6:08 - loss: 1.3879 - acc: 0.250 - ETA: 6:00 - loss: 1.3879 - acc: 0.249 - ETA: 5:52 - loss: 1.3879 - acc: 0.249 - ETA: 5:44 - loss: 1.3880 - acc: 0.249 - ETA: 5:36 - loss: 1.3880 - acc: 0.247 - ETA: 5:28 - loss: 1.3882 - acc: 0.246 - ETA: 5:20 - loss: 1.3881 - acc: 0.246 - ETA: 5:12 - loss: 1.3882 - acc: 0.246 - ETA: 5:04 - loss: 1.3881 - acc: 0.247 - ETA: 4:56 - loss: 1.3881 - acc: 0.247 - ETA: 4:48 - loss: 1.3881 - acc: 0.247 - ETA: 4:40 - loss: 1.3880 - acc: 0.248 - ETA: 4:32 - loss: 1.3881 - acc: 0.246 - ETA: 4:24 - loss: 1.3881 - acc: 0.246 - ETA: 4:15 - loss: 1.3880 - acc: 0.246 - ETA: 4:07 - loss: 1.3880 - acc: 0.246 - ETA: 3:59 - loss: 1.3881 - acc: 0.247 - ETA: 3:51 - loss: 1.3880 - acc: 0.247 - ETA: 3:43 - loss: 1.3879 - acc: 0.247 - ETA: 3:35 - loss: 1.3877 - acc: 0.248 - ETA: 3:28 - loss: 1.3879 - acc: 0.247 - ETA: 3:20 - loss: 1.3878 - acc: 0.246 - ETA: 3:12 - loss: 1.3880 - acc: 0.246 - ETA: 3:04 - loss: 1.3880 - acc: 0.245 - ETA: 2:56 - loss: 1.3880 - acc: 0.246 - ETA: 2:48 - loss: 1.3880 - acc: 0.246 - ETA: 2:40 - loss: 1.3880 - acc: 0.246 - ETA: 2:32 - loss: 1.3880 - acc: 0.246 - ETA: 2:24 - loss: 1.3879 - acc: 0.247 - ETA: 2:16 - loss: 1.3881 - acc: 0.246 - ETA: 2:08 - loss: 1.3881 - acc: 0.246 - ETA: 2:00 - loss: 1.3881 - acc: 0.246 - ETA: 1:52 - loss: 1.3881 - acc: 0.245 - ETA: 1:44 - loss: 1.3881 - acc: 0.246 - ETA: 1:36 - loss: 1.3882 - acc: 0.246 - ETA: 1:28 - loss: 1.3881 - acc: 0.246 - ETA: 1:20 - loss: 1.3881 - acc: 0.247 - ETA: 1:12 - loss: 1.3881 - acc: 0.247 - ETA: 1:04 - loss: 1.3881 - acc: 0.246 - ETA: 57s - loss: 1.3881 - acc: 0.247 - ETA: 49s - loss: 1.3880 - acc: 0.24 - ETA: 41s - loss: 1.3880 - acc: 0.24 - ETA: 33s - loss: 1.3880 - acc: 0.24 - ETA: 25s - loss: 1.3880 - acc: 0.24 - ETA: 17s - loss: 1.3879 - acc: 0.24 - ETA: 8s - loss: 1.3878 - acc: 0.2498 - ETA: 0s - loss: 1.3877 - acc: 0.249 - 875s 88ms/step - loss: 1.3877 - acc: 0.2500 - val_loss: 1.3999 - val_acc: 0.2500\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9912/9912 [==============================] - ETA: 13:28 - loss: 1.3851 - acc: 0.26 - ETA: 13:41 - loss: 1.3862 - acc: 0.25 - ETA: 13:30 - loss: 1.3905 - acc: 0.24 - ETA: 13:21 - loss: 1.3874 - acc: 0.23 - ETA: 13:07 - loss: 1.3893 - acc: 0.24 - ETA: 12:54 - loss: 1.3912 - acc: 0.23 - ETA: 12:41 - loss: 1.3905 - acc: 0.23 - ETA: 12:29 - loss: 1.3904 - acc: 0.23 - ETA: 12:19 - loss: 1.3908 - acc: 0.23 - ETA: 12:09 - loss: 1.3907 - acc: 0.23 - ETA: 12:01 - loss: 1.3900 - acc: 0.23 - ETA: 11:52 - loss: 1.3890 - acc: 0.24 - ETA: 11:43 - loss: 1.3883 - acc: 0.25 - ETA: 11:38 - loss: 1.3883 - acc: 0.25 - ETA: 11:28 - loss: 1.3887 - acc: 0.24 - ETA: 11:17 - loss: 1.3885 - acc: 0.24 - ETA: 11:07 - loss: 1.3882 - acc: 0.25 - ETA: 10:59 - loss: 1.3879 - acc: 0.25 - ETA: 10:49 - loss: 1.3883 - acc: 0.25 - ETA: 10:40 - loss: 1.3883 - acc: 0.25 - ETA: 10:31 - loss: 1.3884 - acc: 0.25 - ETA: 10:22 - loss: 1.3880 - acc: 0.25 - ETA: 10:13 - loss: 1.3876 - acc: 0.25 - ETA: 10:04 - loss: 1.3874 - acc: 0.25 - ETA: 9:56 - loss: 1.3870 - acc: 0.2612 - ETA: 9:47 - loss: 1.3867 - acc: 0.261 - ETA: 9:38 - loss: 1.3871 - acc: 0.259 - ETA: 9:30 - loss: 1.3870 - acc: 0.261 - ETA: 9:22 - loss: 1.3870 - acc: 0.259 - ETA: 9:13 - loss: 1.3869 - acc: 0.260 - ETA: 9:05 - loss: 1.3870 - acc: 0.259 - ETA: 8:56 - loss: 1.3870 - acc: 0.259 - ETA: 8:48 - loss: 1.3871 - acc: 0.257 - ETA: 8:40 - loss: 1.3873 - acc: 0.257 - ETA: 8:31 - loss: 1.3873 - acc: 0.256 - ETA: 8:23 - loss: 1.3871 - acc: 0.255 - ETA: 8:15 - loss: 1.3877 - acc: 0.253 - ETA: 8:07 - loss: 1.3875 - acc: 0.256 - ETA: 7:59 - loss: 1.3874 - acc: 0.256 - ETA: 7:51 - loss: 1.3875 - acc: 0.255 - ETA: 7:42 - loss: 1.3876 - acc: 0.255 - ETA: 7:34 - loss: 1.3873 - acc: 0.257 - ETA: 7:27 - loss: 1.3876 - acc: 0.255 - ETA: 7:19 - loss: 1.3875 - acc: 0.257 - ETA: 7:11 - loss: 1.3874 - acc: 0.258 - ETA: 7:03 - loss: 1.3875 - acc: 0.258 - ETA: 6:54 - loss: 1.3876 - acc: 0.258 - ETA: 6:46 - loss: 1.3875 - acc: 0.257 - ETA: 6:38 - loss: 1.3875 - acc: 0.257 - ETA: 6:30 - loss: 1.3875 - acc: 0.257 - ETA: 6:22 - loss: 1.3874 - acc: 0.257 - ETA: 6:14 - loss: 1.3876 - acc: 0.257 - ETA: 6:06 - loss: 1.3876 - acc: 0.258 - ETA: 5:58 - loss: 1.3878 - acc: 0.256 - ETA: 5:50 - loss: 1.3877 - acc: 0.257 - ETA: 5:42 - loss: 1.3876 - acc: 0.258 - ETA: 5:34 - loss: 1.3874 - acc: 0.258 - ETA: 5:26 - loss: 1.3873 - acc: 0.259 - ETA: 5:18 - loss: 1.3869 - acc: 0.261 - ETA: 5:10 - loss: 1.3871 - acc: 0.261 - ETA: 5:02 - loss: 1.3874 - acc: 0.260 - ETA: 4:54 - loss: 1.3875 - acc: 0.259 - ETA: 4:46 - loss: 1.3875 - acc: 0.258 - ETA: 4:38 - loss: 1.3874 - acc: 0.259 - ETA: 4:30 - loss: 1.3872 - acc: 0.260 - ETA: 4:22 - loss: 1.3873 - acc: 0.260 - ETA: 4:14 - loss: 1.3873 - acc: 0.259 - ETA: 4:06 - loss: 1.3873 - acc: 0.259 - ETA: 3:58 - loss: 1.3872 - acc: 0.260 - ETA: 3:50 - loss: 1.3872 - acc: 0.260 - ETA: 3:42 - loss: 1.3871 - acc: 0.261 - ETA: 3:34 - loss: 1.3871 - acc: 0.261 - ETA: 3:27 - loss: 1.3870 - acc: 0.262 - ETA: 3:19 - loss: 1.3871 - acc: 0.261 - ETA: 3:11 - loss: 1.3872 - acc: 0.260 - ETA: 3:03 - loss: 1.3873 - acc: 0.259 - ETA: 2:55 - loss: 1.3872 - acc: 0.259 - ETA: 2:47 - loss: 1.3872 - acc: 0.260 - ETA: 2:40 - loss: 1.3872 - acc: 0.260 - ETA: 2:32 - loss: 1.3872 - acc: 0.259 - ETA: 2:24 - loss: 1.3872 - acc: 0.259 - ETA: 2:16 - loss: 1.3873 - acc: 0.259 - ETA: 2:08 - loss: 1.3872 - acc: 0.259 - ETA: 2:00 - loss: 1.3871 - acc: 0.261 - ETA: 1:52 - loss: 1.3872 - acc: 0.260 - ETA: 1:44 - loss: 1.3873 - acc: 0.259 - ETA: 1:36 - loss: 1.3872 - acc: 0.260 - ETA: 1:28 - loss: 1.3871 - acc: 0.260 - ETA: 1:20 - loss: 1.3872 - acc: 0.260 - ETA: 1:12 - loss: 1.3871 - acc: 0.260 - ETA: 1:04 - loss: 1.3871 - acc: 0.260 - ETA: 56s - loss: 1.3870 - acc: 0.261 - ETA: 48s - loss: 1.3870 - acc: 0.26 - ETA: 40s - loss: 1.3870 - acc: 0.26 - ETA: 33s - loss: 1.3870 - acc: 0.26 - ETA: 24s - loss: 1.3870 - acc: 0.26 - ETA: 16s - loss: 1.3870 - acc: 0.26 - ETA: 8s - loss: 1.3869 - acc: 0.2620 - ETA: 0s - loss: 1.3870 - acc: 0.262 - 870s 88ms/step - loss: 1.3869 - acc: 0.2621 - val_loss: 1.3895 - val_acc: 0.2504\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.25000 to 0.25040, saving model to CNNWeights.h5\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = batch_size, nb_epoch = nr_iterations, shuffle = True, verbose = 1, validation_data = (x_test, y_test) ,callbacks=[checkpoint,csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.38951599367\n",
      "Test accuracy: 0.250403225806\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose = 0)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.38951599367\n",
      "Test accuracy: 0.250403225806\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose = 0)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.60370731  0.55057102  0.66479295  0.55031967]]\n",
      "0.60\n",
      "0.55\n",
      "0.66\n",
      "0.55\n"
     ]
    }
   ],
   "source": [
    "im = io.imread(\"F:\\\\Work\\\\Python Machine Learning\\\\CLS-Python\\\\CLS-Python\\\\Machine_learning\\\\BloodCells_deepLearning\\\\TEST\\\\NEUTROPHIL\" + \"\\\\\" + \"_0_768.jpeg\", as_grey=True)\n",
    "im = misc.imresize(im , (240, 240))\n",
    "im = np.array(im)\n",
    "im = im.astype('float32')\n",
    "im /= 255\n",
    "im = np.array(im).reshape(-1, 240,240,1)\n",
    "\n",
    "#weights = model.load_weights(saved_weights_name)\n",
    "\n",
    "Results = model.predict(im)\n",
    "print(Results)\n",
    "print(\"%.2f\" % Results[0][0])\n",
    "print(\"%.2f\" % Results[0][1])\n",
    "print(\"%.2f\" % Results[0][2])\n",
    "print(\"%.2f\" % Results[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
